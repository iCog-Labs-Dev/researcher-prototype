"""
Research graph builder module that constructs the LangGraph for autonomous research workflow.
"""

import os
from langgraph.graph import StateGraph, END

from services.nodes.base import ChatState
from config import (
    LANGCHAIN_TRACING_V2,
    LANGCHAIN_API_KEY,
    LANGCHAIN_ENDPOINT,
    LANGCHAIN_PROJECT
)
from services.logging_config import get_logger
from utils.helpers import visualize_langgraph
from utils.error_handling import check_error, route_on_llm_error
# Import research-specific nodes
from services.nodes.research_initializer import research_initializer_node
from services.nodes.research_query_generator import research_query_generator_node
from services.nodes.research_quality_assessor import research_quality_assessor_node
from services.nodes.research_deduplication import research_deduplication_node
from services.nodes.research_storage import research_storage_node
# Import reused nodes from existing graph  
from services.nodes.integrator import integrator_node
from services.nodes.response_renderer import response_renderer_node
# Import multi-source search coordination
from services.nodes.research_source_selector import research_source_selector_node
from services.nodes.source_coordinator import source_coordinator_node
from services.nodes.search_prompt_optimizer import search_prompt_optimizer_node
from services.nodes.search_results_reviewer import search_results_reviewer_node
from services.nodes.evidence_summarizer import evidence_summarizer_node

logger = get_logger(__name__)


def setup_research_tracing():
    """Configure LangSmith tracing for research graph."""
    if LANGCHAIN_TRACING_V2 and LANGCHAIN_API_KEY:
        os.environ["LANGCHAIN_TRACING_V2"] = "true"
        os.environ["LANGCHAIN_ENDPOINT"] = LANGCHAIN_ENDPOINT
        os.environ["LANGCHAIN_API_KEY"] = LANGCHAIN_API_KEY
        os.environ["LANGCHAIN_PROJECT"] = f"{LANGCHAIN_PROJECT}-research"  # Separate project for research
        logger.info(f"üîç LangSmith tracing enabled for research project: {LANGCHAIN_PROJECT}-research")


def create_research_graph():
    """Create a LangGraph graph for orchestrating autonomous research workflow."""
    
    # Configure LangSmith tracing if enabled
    if LANGCHAIN_TRACING_V2 and LANGCHAIN_API_KEY:
        setup_research_tracing()
    
    # Build the research graph
    builder = StateGraph(ChatState)
    
    # Add research-specific nodes
    builder.add_node("research_initializer", research_initializer_node)
    builder.add_node("research_query_generator", research_query_generator_node)
    builder.add_node("research_source_selector", research_source_selector_node)
    
    # Add source coordinator node (handles all search sources internally)
    builder.add_node("source_coordinator", source_coordinator_node)
    # Add shared search optimizer to refine queries before multi-source search
    builder.add_node("search_prompt_optimizer", search_prompt_optimizer_node)
    # Add results reviewer to filter irrelevant items before integration
    builder.add_node("search_results_reviewer", search_results_reviewer_node)
    # Add evidence summarizer to create concise summaries with citations
    builder.add_node("evidence_summarizer", evidence_summarizer_node)
    
    # Add processing nodes
    builder.add_node("integrator", integrator_node)
    builder.add_node("response_renderer", response_renderer_node)
    builder.add_node("research_quality_assessor", research_quality_assessor_node)
    builder.add_node("research_deduplication", research_deduplication_node)
    builder.add_node("research_storage", research_storage_node)
    
    # Define the research workflow
    builder.set_entry_point("research_initializer")
    
    # RInit -> RQGen; then LLM error? after RQGen, RSrcSel, ROpt (yes->END, no->next)
    builder.add_edge("research_initializer", "research_query_generator")
    builder.add_conditional_edges(
        "research_query_generator",
        route_on_llm_error,
        {"continue": "research_source_selector", END: END},
    )
    
    builder.add_conditional_edges(
        "research_source_selector",
        route_on_llm_error,
        {"continue": "search_prompt_optimizer", END: END},
    )
    
    builder.add_conditional_edges(
        "search_prompt_optimizer",
        route_on_llm_error,
        {"continue": "source_coordinator", END: END},
    )
    # RCoord -> RRev -> RSumm -> RInt -> RRend (RRev, RSumm, RInt use check_error; RRend is plain)
    builder.add_edge("source_coordinator", "search_results_reviewer")
    
    builder.add_conditional_edges(
        "search_results_reviewer",
        check_error,
        {"continue": "evidence_summarizer", END: END}
    )
    
    builder.add_conditional_edges(
        "evidence_summarizer",
        check_error,
        {"continue": "integrator", END: END}
    )
    
    builder.add_conditional_edges(
        "integrator",
        check_error,
        {"continue": "response_renderer", END: END}
    )

    builder.add_edge("response_renderer", "research_quality_assessor")
    builder.add_edge("research_quality_assessor", "research_deduplication")
    builder.add_edge("research_deduplication", "research_storage")
    
    # End the graph after storage
    builder.add_edge("research_storage", END)
    
    # Compile the research graph
    research_graph = builder.compile()
    
    logger.info("üî¨ Research graph compiled successfully")
    return research_graph


# Create a singleton instance of the research graph
research_graph = create_research_graph()


def visualize_research_graph(output_file="research_graph.png"):
    """
    Generate a PNG visualization of the research LangGraph.
    
    Args:
        output_file: Path where to save the PNG file.
    
    Returns:
        bool: True if visualization was successful, False otherwise.
    """
    return visualize_langgraph(research_graph, output_file, "Research LangGraph")


def generate_all_graph_visualizations():
    """
    Generate visualizations for both the main chat graph and research graph.
    
    Returns:
        dict: Results of visualization generation for both graphs
    """
    results = {}
    
    print("üî¨ Generating all graph visualizations...")
    
    # Generate research graph visualization
    print("\n1. Generating Research Graph visualization...")
    research_result = visualize_research_graph("research_graph.png")
    results["research_graph"] = {
        "success": research_result,
        "filename": "research_graph.png"
    }
    
    # Generate main chat graph visualization
    try:
        from .chat import visualize_graph
        print("\n2. Generating Main Chat Graph visualization...")
        chat_result = visualize_graph("chat_graph.png")
        results["chat_graph"] = {
            "success": chat_result,
            "filename": "chat_graph.png"
        }
    except ImportError as e:
        print(f"Could not import main graph visualization: {e}")
        results["chat_graph"] = {
            "success": False,
            "error": str(e)
        }
    
    # Summary
    print(f"\nüìä Visualization Summary:")
    for graph_name, result in results.items():
        status = "‚úÖ Success" if result.get("success") else "‚ùå Failed"
        filename = result.get("filename", "N/A")
        print(f"  {graph_name}: {status} - {filename}")
    
    return results


def show_help():
    """Show usage information for the research graph builder."""
    print("""
üî¨ Research Graph Builder - Visualization Tool

USAGE:
  python research.py [option|filename]

OPTIONS:
  --help, -h        Show this help message
  --all             Generate both research graph and main chat graph visualizations
  <filename.png>    Generate research graph with custom filename

EXAMPLES:
  python research.py                          # Generate research_graph.png
  python research.py --all                    # Generate both graphs
  python research.py my_custom_graph.png      # Custom filename
  python research.py --help                   # Show this help

OUTPUT FILES:
  research_graph.png    - Research workflow visualization
  chat_graph.png        - Main chat workflow visualization (with --all)
    """)


# Automatically generate visualization when this module is run directly
if __name__ == "__main__":
    import sys
    
    # Check for command line arguments
    if len(sys.argv) > 1:
        arg = sys.argv[1]
        
        if arg in ["--help", "-h"]:
            # Show help
            show_help()
        elif arg == "--all":
            # Generate both graphs
            generate_all_graph_visualizations()
        else:
            # Use custom filename
            output_file = arg
            print(f"Generating research graph visualization: {output_file}")
            success = visualize_research_graph(output_file)
            if success:
                print(f"‚úÖ Successfully generated: {output_file}")
            else:
                print(f"‚ùå Failed to generate: {output_file}")
    else:
        # Default behavior
        visualize_research_graph() 