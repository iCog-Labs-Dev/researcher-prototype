from typing import Dict, Any
import asyncio

from nodes.base import (
    logger,
    config,
    get_current_datetime_str,
    ChatOpenAI,
    SystemMessage,
    HumanMessage,
    VAGUENESS_DETECTION_PROMPT,
    CLARIFYING_QUESTION_GENERATION_PROMPT,
    QUERY_REFINEMENT_SUGGESTION_PROMPT
)
from models import (
    QueryDisambiguationAnalysis,
    ClarifyingQuestion
)
from llm_models import (
    VaguenessDetection,
    ClarifyingQuestionsOutput,
    QueryRefinementSuggestions
)


class QueryDisambiguationService:
    """Service for analyzing query vagueness and generating clarifying questions using LLM."""
    
    async def analyze_query(self, query: str, context: Dict[str, Any]) -> QueryDisambiguationAnalysis:
        logger.info(f"Query Disambiguation: analyzing '{query[:50]}...'")
        try:
            analysis = await self._llm_analyze_query(query, context)
            logger.info(f"LLM analysis complete - vague: {analysis.is_vague}, confidence: {analysis.confidence_score:.2f}")
            return analysis
        except Exception as e:
            logger.error(f"Error in LLM analysis: {str(e)}")
            return QueryDisambiguationAnalysis(
                is_vague=False,
                confidence_score=0.0,
                vague_indicators=[],
                clarifying_questions=[],
                suggested_refinements=[],
                context_analysis=f"LLM analysis failed: {str(e)}"
            )

    async def _llm_analyze_query(self, query: str, context: Dict[str, Any]) -> QueryDisambiguationAnalysis:
        """Analyze query using specialized prompts in separate LLM calls."""
        llm = ChatOpenAI(
            model=config.ROUTER_MODEL,
            temperature=0.1,
            max_tokens=300,
            api_key=config.OPENAI_API_KEY
        )

        conversation_context = self._build_conversation_context(context)
        memory_context = context.get("memory_context", "")
        memory_section = f"CONVERSATION MEMORY:\n{memory_context}\n\n" if memory_context else ""
        current_time = get_current_datetime_str()

        # Task 1: Detect vagueness
        vagueness_prompt = VAGUENESS_DETECTION_PROMPT.format(
            current_time=current_time,
            memory_context_section=memory_section
        )
        vagueness_messages = [
            SystemMessage(content=vagueness_prompt),
            HumanMessage(content=f"QUERY TO ANALYZE: {query}\n\nCONVERSATION CONTEXT:\n{conversation_context}")
        ]
        
        structured_vagueness_llm = llm.with_structured_output(VaguenessDetection)
        
        vagueness_result = await structured_vagueness_llm.ainvoke(vagueness_messages)
        
        logger.debug(f"Vagueness detection: is_vague={vagueness_result.is_vague}, confidence={vagueness_result.confidence_score:.2f}")

        # Task 2 & 3: Only generate questions and refinements if query is vague
        clarifying_questions = []
        suggested_refinements = []
        
        if vagueness_result.is_vague:
            # Run question generation and refinement suggestions in parallel
            question_task = self._generate_clarifying_questions(
                query, conversation_context, memory_section, current_time, llm
            )
            refinement_task = self._generate_refinement_suggestions(
                query, conversation_context, memory_section, current_time, llm
            )
            
            questions_result, refinements_result = await asyncio.gather(
                question_task,
                refinement_task,
                return_exceptions=True
            )
            
            if isinstance(questions_result, Exception):
                logger.error(f"Error generating clarifying questions: {questions_result}")
            else:
                # Convert to ClarifyingQuestion objects
                for i, question_text in enumerate(questions_result.questions):
                    question_type = questions_result.question_types[i] if i < len(questions_result.question_types) else "open_ended"
                    clarifying_questions.append(ClarifyingQuestion(
                        question=question_text,
                        question_type=question_type,
                        context="Generated by LLM"
                    ))
            
            if isinstance(refinements_result, Exception):
                logger.error(f"Error generating refinements: {refinements_result}")
            else:
                suggested_refinements = refinements_result.suggested_refinements

        # Combine all results into QueryDisambiguationAnalysis
        return QueryDisambiguationAnalysis(
            is_vague=vagueness_result.is_vague,
            confidence_score=vagueness_result.confidence_score,
            vague_indicators=vagueness_result.vague_indicators,
            clarifying_questions=clarifying_questions,
            suggested_refinements=suggested_refinements,
            context_analysis=vagueness_result.context_analysis
        )

    async def _generate_clarifying_questions(
        self, query: str, conversation_context: str, memory_section: str, current_time: str, llm: ChatOpenAI
    ) -> ClarifyingQuestionsOutput:
        """Generate clarifying questions using specialized prompt."""
        question_prompt = CLARIFYING_QUESTION_GENERATION_PROMPT.format(
            current_time=current_time,
            memory_context_section=memory_section
        )
        question_messages = [
            SystemMessage(content=question_prompt),
            HumanMessage(content=f"VAGUE QUERY: {query}\n\nCONVERSATION CONTEXT:\n{conversation_context}")
        ]
        
        structured_question_llm = llm.with_structured_output(ClarifyingQuestionsOutput)
        return await structured_question_llm.ainvoke(question_messages)

    async def _generate_refinement_suggestions(
        self, query: str, conversation_context: str, memory_section: str, current_time: str, llm: ChatOpenAI
    ) -> QueryRefinementSuggestions:
        """Generate query refinement suggestions using specialized prompt."""
        
        refinement_prompt = QUERY_REFINEMENT_SUGGESTION_PROMPT.format(
            current_time=current_time,
            memory_context_section=memory_section
        )
        refinement_messages = [
            SystemMessage(content=refinement_prompt),
            HumanMessage(content=f"VAGUE QUERY: {query}\n\nCONVERSATION CONTEXT:\n{conversation_context}")
        ]
        
        structured_refinement_llm = llm.with_structured_output(QueryRefinementSuggestions)
        return await structured_refinement_llm.ainvoke(refinement_messages)

    def _build_conversation_context(self, context: Dict[str, Any]) -> str:
        messages = context.get("messages", [])
        if not messages:
            return "No previous conversation context."
        recent_messages = messages[-3:]
        
        context_lines = [
            f"{'User' if msg.role == 'user' else 'Assistant'}: {msg.content}"
            for msg in recent_messages if hasattr(msg, "content")
        ]
        return "\n".join(context_lines) if context_lines else "No recent conversation context."
